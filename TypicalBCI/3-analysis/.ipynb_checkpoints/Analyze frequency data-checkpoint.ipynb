{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne.channels import read_montage\n",
    "from mne import Epochs, find_events\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "from mne.time_frequency import psd_welch\n",
    "\n",
    "# visualization stuff\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/anger-amuse-100trials-6s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #check the size of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate 32 participants data\n",
    "#df = pd.concat([df] * 32)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.Marker==2].shape) #anger\n",
    "print(df[df.Marker==1].shape) #amusements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Unnamed: 1':'Fp1',\n",
    "                          'Unnamed: 2':'Fp2',\n",
    "                          'Unnamed: 3':'F3',\n",
    "                          'Unnamed: 4':'F4',}, \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"timestamps\", \"Unnamed: 5\", \"Unnamed: 6\", \"Unnamed: 7\", \"Unnamed: 8\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transform Data into Raw MNE object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = 250\n",
    "ch_names = list(df.columns)\n",
    "ch_types = ['eeg'] * (len(df.columns) - 1) + ['stim']\n",
    "ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_types, ch_names, ten_twenty_montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T  #mne looks at the tranpose() format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:-1] *= -1e-6  #convert from uVolts to Volts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq,\n",
    "                  montage=ten_twenty_montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.RawArray(df, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try plotting the raw data of its power spectral density\n",
    "raw.plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notch Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some artifacts are restricted to certain frequencies and can therefore be fixed by filtering. An artifact that typically affects only some frequencies is due to the power line.\n",
    "\n",
    "Power-line noise is a noise created by the electrical network. It is composed of sharp peaks at 50Hz (or 60Hz depending on your geographical location). Some peaks may also be present at the harmonic frequencies, i.e. the integer multiples of the power-line frequency, e.g. 100Hz, 150Hz, … (or 120Hz, 180Hz, …).\n",
    "\n",
    "Remove the 50Hz power line noise in Thailand.  We will also be remove its harmonics, i.e., 100Hz, 150Hz, etc.  Since our signal is 125Hz (250Hz / 2 according to Nyquist Theorem), we shall run the harmonics until 125 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.notch_filter(np.arange(50, 125, 50), filter_length='auto', phase='zero') #250/2 based on Nyquist Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe that the 50Hz noise is now gone, yay!\n",
    "raw.plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent components analysis (ICA) is a technique for estimating independent source signals from a set of recordings in which the source signals were mixed together in unknown ratios. A common example of this is the problem of blind source separation: with 3 musical instruments playing in the same room, and 3 microphones recording the performance (each picking up all 3 instruments, but at varying levels), can you somehow “unmix” the signals recorded by the 3 microphones so that you end up with a separate “recording” isolating the sound of each instrument?\n",
    "\n",
    "It is not hard to see how this analogy applies to EEG/MEG analysis: there are many “microphones” (sensor channels) simultaneously recording many “instruments” (blinks, heartbeats, activity in different areas of the brain, muscular activity from jaw clenching or swallowing, etc). As long as these various source signals are statistically independent and non-gaussian, it is usually possible to separate the sources using ICA, and then re-construct the sensor signals after excluding the sources that are unwanted.\n",
    "\n",
    "MNE-Python implements three different ICA algorithms: fastica (the default), picard, and infomax. FastICA and Infomax are both in fairly widespread use; Picard is a newer (2017) algorithm that is expected to converge faster than FastICA and Infomax, and is more robust than other algorithms in cases where the sources are not completely independent, which typically happens with real EEG/MEG data\n",
    "\n",
    "The ICA interface in MNE-Python is similar to the interface in scikit-learn: some general parameters are specified when creating an ICA object, then the ICA object is fit to the data using its fit() method. The results of the fitting are added to the ICA object as attributes that end in an underscore (_), such as ica.mixing_matrix_ and ica.unmixing_matrix_. After fitting, the ICA component(s) that you want to remove must be chosen, and the ICA fit must then be applied to the Raw or Epochs object using the ICA object’s apply() method.\n",
    "\n",
    "After visualizing the Independent Components (ICs) and excluding any that capture artifacts you want to repair, the sensor signal can be reconstructed using the ICA object’s apply() method. By default, signal reconstruction uses all of the ICs (less any ICs listed in ICA.exclude) plus all of the PCs that were not included in the ICA decomposition (i.e., the “PCA residual”). If you want to reduce the number of components used at the reconstruction stage, it is controlled by the n_pca_components parameter (which will in turn reduce the rank of your data; by default n_pca_components = max_pca_components resulting in no additional dimensionality reduction). \n",
    "\n",
    "Before we run the ICA, an important step is filtering the data to remove low-frequency drifts, which can negatively affect the quality of the ICA fit. The slow drifts are problematic because they reduce the independence of the assumed-to-be-independent sources (e.g., during a slow upward drift, the neural, heartbeat, blink, and other muscular sources will all tend to have higher values), making it harder for the algorithm to find an accurate solution. **A high-pass filter with 1 Hz cutoff frequency is recommended.** However, because filtering is a linear operation, the ICA solution found from the filtered signal can be applied to the unfiltered signal, so we’ll keep a copy of the unfiltered Raw object around so we can apply the ICA solution to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering to remove slow drifts; also make copy of raw for later signal reconstruction\n",
    "filt_raw = raw.copy()\n",
    "filt_raw.load_data().filter(l_freq=1., h_freq=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_raw.plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to set up and fit the ICA. We’ll run ICA with n_components=4 since we only have 4 channels.  If we have more, then we may pick a smaller number.  How small?  There is no answer, but clearly, we want to try until we can see the eye, muscle components, etc.  But too large the component will slow the fit() process.\n",
    "\n",
    "ICA fitting is not deterministic (e.g., the components may get a sign flip on different runs, or may not always be returned in the same order), so we’ll also specify a random seed so that we get identical results each time this tutorial is built by our web servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and fit the ICA\n",
    "ica = mne.preprocessing.ICA(n_components=4, random_state=97, max_iter=800)\n",
    "ica.fit(filt_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the ICs to see what they captured. plot_sources() will show the time series of the ICs. Note that in our call to plot_sources() we can use the original, unfiltered Raw object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_raw.load_data()\n",
    "ica.plot_sources(filt_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can pretty clearly see that the first component (ICA000) captures some noise(for more info on visually identifying Independent Components, this EEGLAB tutorial (https://labeling.ucsd.edu/tutorial/labels) is a good resource). \n",
    "\n",
    "Typically, while brain activity exists at frequencies that surpass 200 Hz, low frequencies are typically the only ones synchronous enough to show up in EEG. Therefore, **brain components** tend to have diminishing power at higher frequencies.  In other words, most higher frequencies data are usually artifacts.   Additionally, for all **eye components**, the power spectrum will vary due to experiments and people, but generally most of the power will reside at frequencies below 5 Hz as people do not usually move their eye faster than that. **Muscle components** are mostly concentrated in higher frequences (20Hz and above).  These components can still be dipolar, but will be located outside the skull, i.e., shallow source as seen in topography.\n",
    "\n",
    "We can also visualize the scalp field distribution of each component using plot_components(). These are interpolated based on the values in the ICA unmixing matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plots above it’s fairly obvious that ICA000 is capturing our eye blinks due to the eye areas intensity but there are additional ways visualize them anyway just to be sure. \n",
    "\n",
    "We can alo plot an overlay of the original signal against the reconstructed signal with the artifactual ICs excluded, using plot_overlay():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test excluding 0 component\n",
    "ica.plot_overlay(filt_raw, exclude=[0], picks='eeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test excluding 1 component\n",
    "ica.plot_overlay(filt_raw, exclude=[0, 1], picks='eeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot some diagnostics of each IC using plot_properties():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_properties(filt_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we’re certain which components we want to exclude, we can specify that manually by setting the **ica.exclude** attribute. Similar to marking bad channels, merely setting ica.exclude doesn’t do anything immediately (it just adds the excluded ICs to a list that will get used later when it’s needed). Once the exclusions have been set, ICA methods like plot_overlay() will exclude those component(s) even if no exclude parameter is passed, and the list of excluded components will be preserved when using mne.preprocessing.ICA.save() and mne.preprocessing.read_ica()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.exclude = [0,1] #we want to cut down the 0 component, then inverse back to the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the exclusions have been set, we can reconstruct the sensor signals with artifacts removed using the apply() method (remember, we’re applying the ICA solution from the filtered data to the original unfiltered signal). Plotting the original raw data alongside the reconstructed data shows that the blink artifacts are repaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ica.apply() changes the Raw object in-place, so let's make a copy first:\n",
    "reconst_raw = filt_raw.copy()\n",
    "ica.apply(reconst_raw)\n",
    "\n",
    "regexp = r'(F)'\n",
    "artifact_picks = mne.pick_channels_regexp(filt_raw.ch_names, regexp=regexp)\n",
    "\n",
    "filt_raw.plot(order=artifact_picks, n_channels=len(artifact_picks))\n",
    "reconst_raw.plot(order=artifact_picks, n_channels=len(artifact_picks))\n",
    "\n",
    "reconst_raw.plot_psd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Band pass filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will skip this part since we are interested in all bands of frequency.  However, if we are working on ERP (e.g., P300, SSVEP, N170), then we need to filter out all frequences between 1 and 30hz in order to increase our ability to detect them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter code looks like this\n",
    "#raw.filter(1,30, method='iir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Power spectral analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the poweer spectral density (PSD) using \n",
    "# the MNE psd_welch function\n",
    "# (this is simply a wrapper on scipy.signal.welch\n",
    "#  that adds compatbility for MNE data types)\n",
    "\n",
    "#this pick is optional but in case you want\n",
    "#to pick certain channel\n",
    "picks = mne.pick_types(reconst_raw.info, eeg=True, exclude='bads')\n",
    "\n",
    "#you can also define manually the picks\n",
    "#picks = [0,1]  here i am choosing the first two channels\n",
    "\n",
    "reconst_raw.plot_psd(n_fft=2048, picks=picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting psd manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd, freqs = psd_welch(reconst_raw, picks=[0,1])  #welch is a common method of computing power spectral density\n",
    "psd1, freqs1 = psd_welch(reconst_raw, picks=[2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get mean\n",
    "psd = 10 * np.log10(psd)\n",
    "psd_mean = psd.mean(0)\n",
    "psd_std = psd.std(0)\n",
    "\n",
    "psd1 = 10 * np.log10(psd1)\n",
    "psd1_mean = psd1.mean(0)\n",
    "psd1_std = psd1.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.plot(freqs, psd_mean, color='green', label='Fp1-2')\n",
    "ax.plot(freqs1, psd1_mean, color='red', label='F3-4')\n",
    "\n",
    "ax.set(title='PSD', xlabel='Freq', ylabel='PSD(dB)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd, freqs = psd_welch(reconst_raw)  #welch is a common method of computing power spectral density\n",
    "df_psd = pd.DataFrame(psd, columns=freqs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd.tail() #ends at Nyquist Theroem frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd.columns = reconst_raw.ch_names[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd.index.names = ['freq']  #make the plot looks nice with name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd.columns.names = ['chan'] #make the plot looks nice with name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7, 3))\n",
    "df_psd.plot(logy=True, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the delta, theta, alpha, beta, and gamma bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the conventional EEG frequency band names and ranges\n",
    "\n",
    "freqs = ['delta', 'theta', 'alpha', 'beta', 'lowgamma', 'midgamma']\n",
    "\n",
    "freq_bands = dict(delta = [0.5,2],\n",
    "                  theta = [4,8], \n",
    "                  alpha1 = [8,10],\n",
    "                  alpha2 = [10,12],\n",
    "                  beta = [12,20],\n",
    "                  lowgamma=[20,30],\n",
    "                  midgamma=[30,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bands.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the power within each of these bands\n",
    "\n",
    "psd_fb = {}\n",
    "for band_name, (rlow,rhigh) in freq_bands.items():\n",
    "    psd_fb[band_name] = df_psd.loc[rlow:rhigh].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psd_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in pandas df\n",
    "df_psd_fb = pd.DataFrame(psd_fb)\n",
    "df_psd_fb = df_psd_fb.T\n",
    "df_psd_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psd_fb.plot(kind='bar', logy=True) #logy to scale the delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting certain frequency\n",
    "chans = ['Fp1', 'Fp2', 'F3', 'F4']\n",
    "df_psd_fb[chans].loc['alpha1'].plot(kind='bar', logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Epoching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will chunk (epoch) the data into segments representing the data 1s to 6ss after each stimulus. No baseline correction is needed (signal is filtered) and we will reject every epoch where the amplitude of the signal exceeded 100 uV, which should be mostly eye blinks in case our ICA did not detect them (it should, theoretically...right?).\n",
    "\n",
    "**Sample drop % is an important metric representing how noisy our data set was**. If this is greater than 20%, consider ensuring that signal variances is very low in the raw EEG viewer and collecting more data\n",
    "\n",
    "Here, try remove the ICA.  You will find the sample drop % to be very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = find_events(reconst_raw)\n",
    "event_id = {'Amusement': 1, 'Anger' : 2}\n",
    "\n",
    "#reject_criteria = dict(mag=4000e-15,     # 4000 fT\n",
    "#                       grad=4000e-13,    # 4000 fT/cm\n",
    "#                       eeg=150e-6,       # 150 μV\n",
    "#                       eog=250e-6)       # 250 μV\n",
    "\n",
    "reject_criteria = dict(eeg=150e-6)\n",
    "\n",
    "#here I specify 1s to 3s after the stimulus\n",
    "#this one requires expertise and paper reading\n",
    "epochs = Epochs(reconst_raw, events=events, event_id=event_id, \n",
    "                tmin=1, tmax=3, baseline=None, preload=True, \n",
    "                reject=reject_criteria,verbose=False, picks=[0,1,2,3])\n",
    "print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid biasing our signals, we’ll use equalize_event_counts() first to randomly sample epochs from each condition to match the number of epochs present in the condition with the fewest good epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_we_care_about = ['Amusement', 'Anger']\n",
    "epochs.equalize_event_counts(conds_we_care_about)  # this operates in-place\n",
    "amuse_epochs = epochs['Amusement']\n",
    "anger_epochs = epochs['Anger']\n",
    "#del raw, epochs  # free up memory - here I am not gonna do that\n",
    "\n",
    "print(amuse_epochs)\n",
    "print(anger_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs['Anger'].plot_image(picks=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Power Spectral Analysis For Anger vs. Amusements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "axes = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # left, bottom, width, height (range 0 to 1)\n",
    "\n",
    "psd1, freq1 = psd_welch(epochs['Anger'], n_fft=1028, n_per_seg=256 * 3)\n",
    "psd2, freq2 = psd_welch(epochs['Amusement'], n_fft=1028, n_per_seg=256 * 3)\n",
    "\n",
    "logpsd1 = 10 * np.log10(psd1)\n",
    "logpsd2 = 10 * np.log10(psd2)\n",
    "\n",
    "log_psd1_mean = logpsd1.mean(0)\n",
    "log_psd1_std = logpsd1.mean(0)\n",
    "\n",
    "log_psd2_mean = logpsd2.mean(0)\n",
    "log_psd2_std = logpsd2.mean(0)\n",
    "\n",
    "axes.plot(freq1, log_psd1_mean[[0, 3], :].mean(0), color='b', label='Anger')\n",
    "axes.plot(freq2, log_psd2_mean[[0, 3], :].mean(0), color='r', label='Amusement')\n",
    "\n",
    "axes.set_title('Fp1, Fp2, F3, F4')\n",
    "axes.set_ylabel('Power Spectral Density (dB)')\n",
    "axes.set_xlim((2, 50))\n",
    "axes.set_ylim((-150, -110))\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging into band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anger dataFrame\n",
    "df_psd1 = pd.DataFrame(psd1.mean(0), columns=freq1).T\n",
    "df_psd1.columns = reconst_raw.ch_names[:4]\n",
    "df_psd1.index.names = ['freq']  #make the plot looks nice with name\n",
    "df_psd1.columns.names = ['chan'] #make the plot looks nice with name\n",
    "\n",
    "psd_fb1 = {}\n",
    "for band_name, (rlow,rhigh) in freq_bands.items():\n",
    "    psd_fb1[band_name] = df_psd1.loc[rlow:rhigh].mean(axis=0)\n",
    "\n",
    "# Put in pandas df\n",
    "df_psd_fb1 = pd.DataFrame(psd_fb1).T\n",
    "df_psd_fb1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amusement dataFrame\n",
    "df_psd2 = pd.DataFrame(psd2.mean(0), columns=freq2).T\n",
    "df_psd2.columns = reconst_raw.ch_names[:4]\n",
    "df_psd2.index.names = ['freq']  #make the plot looks nice with name\n",
    "df_psd2.columns.names = ['chan'] #make the plot looks nice with name\n",
    "df_psd2\n",
    "\n",
    "psd_fb2 = {}\n",
    "for band_name, (rlow,rhigh) in freq_bands.items():\n",
    "    psd_fb2[band_name] = df_psd2.loc[rlow:rhigh].mean(axis=0)\n",
    "\n",
    "# Put in pandas df\n",
    "df_psd_fb2 = pd.DataFrame(psd_fb2)\n",
    "df_psd_fb2 = df_psd_fb2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "index = np.arange(len(df_psd_fb1.mean(axis=1)))\n",
    "bar_width = 0.35\n",
    "opacity = 0.8\n",
    "\n",
    "anger = plt.bar(index, df_psd_fb1.mean(axis=1), bar_width, alpha=opacity,\n",
    "                color='b', label='Anger', log=True)\n",
    "\n",
    "amusement = plt.bar(index + bar_width, df_psd_fb2.mean(axis=1), bar_width, alpha=opacity,\n",
    "                color='g', label='Amusement', log=True)\n",
    "\n",
    "plt.xlabel('Band')\n",
    "plt.ylabel('Power')\n",
    "plt.title('Band analysis')\n",
    "plt.xticks(index + bar_width, ('delta', 'theta', 'alpha1', \n",
    "                               'alpha2', 'beta', 'l-gamma', 'm-gamma'))\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stage is essential and probably the most important part in all machine learning approches, because a good classifier is useless without good features.  To extract the features, we are going to use a bank of band-pass filters, namely 6-10Hz, 8-12Hz, etc. and then apply a Common Spatial Pattern on each set of filtered EEG signals.  CSP is a spatial filter that allows reducing hte number of EEG channels by selecting those which have maximum differences in variance.  Each pair of band-pass and CSP filters computes the CSP features, which are specific to the band-pass frequency range.\n",
    "\n",
    "There are other ways to extract features, such as Wavelet Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_list = []\n",
    "band_width = 4  #e.g., 8-12Hz\n",
    "band_overlap = 2 #e.g., 8-12Hz, 10-14Hz\n",
    "step = band_width - band_overlap  \n",
    "\n",
    "low_freq = 4\n",
    "n_filters = 23 #e.g., 23 filters in the bank until 50Hz\n",
    "bands = range(low_freq, low_freq + n_filters * step, step)\n",
    "\n",
    "reconst_raw_list = []\n",
    "for low in bands:\n",
    "    reconst_raw_list.append(reconst_raw.copy().filter(low, low+band_width, method='iir'))\n",
    "\n",
    "# Extract epochs\n",
    "epoch_list = []\n",
    "\n",
    "for reconst_raw_bank in reconst_raw_list:\n",
    "    events = find_events(reconst_raw_bank)\n",
    "    event_id = {'Amusement': 1, 'Anger': 2}\n",
    "    \n",
    "    tmin = 1\n",
    "    tmax = 3\n",
    "    \n",
    "    #from 1 to 3 s after stimulus onset, to avoid classifying the ERP)\n",
    "    epoch = (Epochs(reconst_raw_bank, events=events, event_id=event_id, tmin=tmin, \n",
    "                     tmax=tmax, baseline=None, reject={'eeg': 150e-6}, \n",
    "                     preload=True, verbose=False))\n",
    "    epoch_list.append(epoch)\n",
    "    print('sample drop %: ', (1 - len(epoch.events)/len(events)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will loop through all the filter banks and perform classification.  We will use different machine learning pipelines to classify  based on the data we collected. The common pattern here is\n",
    "\n",
    "- **CSP + Classifier** :  Common Spatial Patterns + Regularized Linear Discriminat Analysis. This is a very common EEG analysis pipeline.\n",
    "- **Cov + MDM**: Covariance + MDM. A very simple, yet effective (for low channel count), Riemannian geometry classifier.\n",
    "- **Cov + TS** :  Covariance + Tangent space mapping. One of the most reliable Riemannian geometry-based pipelines.\n",
    "\n",
    "**CSP** is a feature extraction approach to create discriminated features based on maximal variances\n",
    "\n",
    "Evaluation is done through cross-validation, with area-under-the-curve (AUC) as metric (AUC is probably the best metric for binary and unbalanced classification problem)\n",
    "\n",
    "*Note: because we're doing machine learning here, the following cell may take a while to complete*\n",
    "\n",
    "*Note: Scikit-learn API provides functionality to chain transformers and estimators by using sklearn.pipeline.Pipeline. We can construct decoding pipelines and perform cross-validation and grid-search. However scikit-learn transformers and estimators generally expect 2D data (n_samples * n_features), whereas MNE transformers typically output data with a higher dimensionality (e.g. n_samples * n_channels * n_times). A Vectorizer or CSP therefore needs to be applied between the MNE and the scikit-learn steps:\n",
    "\n",
    "First, let me define the method for running the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from mne.decoding import CSP, Vectorizer\n",
    "from pyriemann.classification import MDM\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.estimation import Covariances\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def modeling(X, y, low_freq, epoch):\n",
    "    clfs = OrderedDict()\n",
    "    \n",
    "    lda = LDA(shrinkage='auto', solver='eigen') #Regularized LDA\n",
    "    svc = SVC()\n",
    "    lr = LogisticRegression()\n",
    "    knn = KNeighborsClassifier(n_neighbors=3) #you would want to optimize\n",
    "    nb = GaussianNB()\n",
    "    rf = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "    mdm = MDM()\n",
    "    ts = TangentSpace()\n",
    "    vec = Vectorizer()\n",
    "    scale = Scaler(epoch.info)  #by default, CSP already does this, but if you use Vectorizer, you hve to do it before Vectorizing\n",
    "    csp = CSP(n_components=3, reg=0.3) #feature extraction, reg is used when data is not PD (positive definite)\n",
    "\n",
    "    clfs['Vectorizer + LDA'] = Pipeline([('Vectorizer', vec), ('Model', lda)])\n",
    "    clfs['CSP + LDA'] = Pipeline([('CSP', csp), ('Model', lda)])\n",
    "    clfs['CSP + SVC'] = Pipeline([('CSP', csp), ('Model', svc)])\n",
    "    #clfs['CSP + LR'] = Pipeline([('CSP', csp), ('Model', lr)])\n",
    "    #clfs['CSP + KNN'] = Pipeline([('CSP', csp), ('Model', knn)])\n",
    "    #clfs['CSP + NB'] = Pipeline([('CSP', csp), ('Model', nb)])\n",
    "    #clfs['CSP + RF'] = Pipeline([('CSP', csp), ('Model', rf)])\n",
    "    #clfs['Cov + MDM'] = Pipeline([('Cov', Covariances('oas')), ('Model', mdm)]) #oas is needed for non-PD matrix\n",
    "    #clfs['Cov + TS'] = Pipeline([('Cov', Covariances('oas')), ('Model', ts)]) #oas is needed for non-PD matrix\n",
    "    #not sure why TS is not working....\n",
    "\n",
    "    auc = []\n",
    "    methods = []\n",
    "\n",
    "    # define cross validation \n",
    "    cv = StratifiedShuffleSplit(n_splits=20, test_size=0.25, \n",
    "                            random_state=42)\n",
    "\n",
    "    for m in clfs:\n",
    "        print(\"+\", end=\"\") #to know it's working, no newline\n",
    "        try:\n",
    "            res = cross_val_score(clfs[m], X, y, scoring='roc_auc', \n",
    "                              cv=cv, n_jobs=-1)\n",
    "            auc.extend(res)\n",
    "            methods.extend([m]*len(res))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    results = pd.DataFrame(data=auc, columns=['AUC'])\n",
    "    results['Method'] = methods\n",
    "    \n",
    "    figure = plt.figure(figsize=[8,4])\n",
    "    plt.title(\"%d - %d Hz\" %(low_freq, low_freq + band_width))\n",
    "    sns.barplot(data=results, x='AUC', y='Method')\n",
    "    plt.xlim(0.4, 1)    \n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through each filter bank of epoch and perform modeling\n",
    "#this will take around 10 minutes, so sip a coffee and wait\n",
    "for epoch in epoch_list:\n",
    "    epoch.pick_types(eeg=True)\n",
    "    X = epoch.get_data() * 1e6  #n_epochs * n_channel * n_time_samples  \n",
    "    #CSP will take in data in this form and create features of 2d\n",
    "    times = epoch.times\n",
    "    y = epoch.events[:, -1]\n",
    "    modeling(X, y, low_freq, epoch)\n",
    "    low_freq = low_freq + 2  #for only plotting purpose\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Inverse Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can estimate the origins of the evoked activity by projecting the sensor data into this subject’s source space (a set of points either on the cortical surface or within the cortical volume of that subject, as estimated by structural MRI scans). MNE-Python supports lots of ways of doing this (dynamic statistical parametric mapping, dipole fitting, beamformers, etc.); here we’ll use minimum-norm estimation (MNE) to generate a continuous map of activation constrained to the cortical surface. MNE uses a linear inverse operator to project EEG+MEG sensor measurements into the source space. The inverse operator is computed from the forward solution for this subject and an estimate of the covariance of sensor measurements. For this tutorial we’ll skip those computational steps and load a pre-computed inverse operator from disk (it’s included with the sample data). Because this “inverse problem” is underdetermined (there is no unique solution), here we further constrain the solution by providing a regularization parameter specifying the relative smoothness of the current estimates in terms of a signal-to-noise ratio (where “noise” here is akin to baseline activity level across all of cortex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
